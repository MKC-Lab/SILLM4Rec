{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data_Processor:\n",
    "    def __init__(self, raw_meta_data_path, raw_review_data_path, raw_interaction_data_path):\n",
    "        self.raw_meta_data_path = raw_meta_data_path  # jsonl file\n",
    "        self.raw_review_data_path = raw_review_data_path  # jsonl file\n",
    "        self.raw_interaction_data_path = raw_interaction_data_path  # csv file\n",
    "\n",
    "    def generate_mapping(self, save_dir):\n",
    "        \"\"\"\n",
    "        Generate mapping about asin, user_id, parent_asin, etc.\n",
    "        :param save_dir: str, the directory to save the mapping files\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(self.raw_interaction_data_path)\n",
    "        user_id = data['user_id'].unique()\n",
    "        parent_asin = data['parent_asin'].unique()\n",
    "        user_id_2_index = {user_id[i]: i for i in range(len(user_id))}\n",
    "        parent_asin_2_index = {parent_asin[i]: i for i in range(len(parent_asin))}\n",
    "        index_2_user_id = {i: user_id[i] for i in range(len(user_id))}\n",
    "        index_2_parent_asin = {i: parent_asin[i] for i in range(len(parent_asin))}\n",
    "        with open(os.path.join(save_dir, 'user_id_2_index.json'), 'w') as f:\n",
    "            json.dump(user_id_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "            json.dump(parent_asin_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_user_id.json'), 'w') as f:\n",
    "            json.dump(index_2_user_id, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "            json.dump(index_2_parent_asin, f)\n",
    "\n",
    "    def print_jsonl_data_keys(self, path):\n",
    "        \"\"\"\n",
    "        Print the keys of jsonl data\n",
    "        :param path: str, the path of the jsonl file\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                print(list(data.keys()))\n",
    "                break\n",
    "\n",
    "    def transform_meta_jsonl_to_parquet(self, mapping_dir, save_dir):\n",
    "        parent_asin_2_index = json.load(open(os.path.join(mapping_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "\n",
    "        data = []\n",
    "        parent_asin = parent_asin_2_index.keys()\n",
    "        # keep the columns we need\n",
    "        # all: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'parent_asin', 'bought_together']\n",
    "        # needs: ['main_category', 'title', 'average_rating', 'rating_number', 'price', 'images', 'store', 'categories', 'details', 'parent_asin']\n",
    "        with open(self.raw_meta_data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['parent_asin'] in parent_asin:\n",
    "                    data.append(json.loads(line))\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data[['main_category', 'title', 'average_rating', 'rating_number', 'price', 'images', 'store', 'categories', 'details', 'parent_asin']]\n",
    "        # keep the data whose parent_asin is in the interaction data\n",
    "        data = data[data['parent_asin'].isin(parent_asin)]\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: parent_asin_2_index[x])\n",
    "        data.to_parquet(os.path.join(save_dir, 'meta_data.parquet'))\n",
    "\n",
    "    def transform_review_jsonl_to_parquet(self, mapping_dir, save_dir):\n",
    "        user_id_2_index = json.load(open(os.path.join(mapping_dir, 'user_id_2_index.json'), 'r'))\n",
    "        parent_asin_2_index = json.load(open(os.path.join(mapping_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "        data = []\n",
    "        user_id = set(user_id_2_index.keys())\n",
    "        parent_asin = set(parent_asin_2_index.keys())\n",
    "        # keep the columns we need\n",
    "        # all: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
    "        # needs: ['parent_asin', 'user_id', 'text', 'timestamp', 'rating']\n",
    "        with open(self.raw_review_data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['user_id'] in user_id and json.loads(line)['parent_asin'] in parent_asin:\n",
    "                    data.append(json.loads(line))\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data[['parent_asin', 'user_id', 'text', 'timestamp', 'rating']]\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: parent_asin_2_index[x])\n",
    "        data['user_id'] = data['user_id'].apply(lambda x: user_id_2_index[x])\n",
    "        data.to_parquet(os.path.join(save_dir, 'review_data.parquet'))\n",
    "\n",
    "    def generate_test_data(self, save_dir):\n",
    "        user_id_2_index = json.load(open(os.path.join(save_dir, 'user_id_2_index.json'), 'r'))\n",
    "        parent_asin_2_index = json.load(open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "        data = pd.read_csv(self.raw_interaction_data_path)\n",
    "        data['user_id'] = data['user_id'].apply(lambda x: user_id_2_index[x])\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: parent_asin_2_index[x])\n",
    "        data['history'] = data['history'].apply(lambda x: [parent_asin_2_index[i] for i in x.split()])\n",
    "        data.to_parquet(os.path.join(save_dir, 'test_data.parquet'))\n",
    "\n",
    "\n",
    "raw_data_processor = Raw_Data_Processor(\n",
    "    raw_meta_data_path='raw_data/meta_Baby_Products.jsonl',\n",
    "    raw_review_data_path='raw_data/Baby_Products.jsonl',\n",
    "    raw_interaction_data_path='raw_data/Baby_Products.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data_Processor_to_100user:\n",
    "    def __init__(self, raw_meta_data_path, raw_review_data_path, test_interaction_data_path):\n",
    "        self.raw_meta_data_path = raw_meta_data_path  # jsonl file\n",
    "        self.raw_review_data_path = raw_review_data_path  # jsonl file\n",
    "        self.test_interaction_data_path = test_interaction_data_path  # csv file\n",
    "\n",
    "    def generate_mapping(self, save_dir):\n",
    "        \"\"\"\n",
    "        Generate mapping about asin, user_id, parent_asin, etc.\n",
    "        Here we only keep the data of random 100 users\n",
    "        :param save_dir: str, the directory to save the mapping files\n",
    "        \"\"\"\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "        user_id = data['user_id'].unique()\n",
    "        final_user_id = random.sample(list(user_id), 100)\n",
    "        final_parent_asin = []\n",
    "        for i in range(100):\n",
    "            final_parent_asin += [data[data['user_id'] == final_user_id[i]]['parent_asin'].values[0]] + str(data[data['user_id'] == final_user_id[i]]['history'].values[0]).split()\n",
    "        final_parent_asin = list(set(final_parent_asin))\n",
    "        final_user_id_2_index = {final_user_id[i]: i for i in range(len(final_user_id))}\n",
    "        final_parent_asin_2_index = {final_parent_asin[i]: i for i in range(len(final_parent_asin))}\n",
    "        index_2_final_user_id = {i: final_user_id[i] for i in range(len(final_user_id))}\n",
    "        index_2_final_parent_asin = {i: final_parent_asin[i] for i in range(len(final_parent_asin))}\n",
    "        with open(os.path.join(save_dir, 'user_id_2_index.json'), 'w') as f:\n",
    "            json.dump(final_user_id_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "            json.dump(final_parent_asin_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_user_id.json'), 'w') as f:\n",
    "            json.dump(index_2_final_user_id, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "            json.dump(index_2_final_parent_asin, f)\n",
    "\n",
    "    def generate_test_mapping(self, save_dir, num, train_user):\n",
    "        # generate the mapping of the test data, the test data is the data of the users who are not in the train data\n",
    "        final_user_id_2_index = train_user\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "        user_id = data['user_id'].unique()\n",
    "        final_user_id = [i for i in user_id if i not in final_user_id_2_index.keys()]\n",
    "        final_parent_asin = []\n",
    "        final_user_id = random.sample(final_user_id, num)\n",
    "        print('取出的测试用户数：', len(final_user_id))\n",
    "        for i in range(num):\n",
    "            final_parent_asin += [data[data['user_id'] == final_user_id[i]]['parent_asin'].values[0]] + str(data[data['user_id'] == final_user_id[i]]['history'].values[0]).split()\n",
    "        final_parent_asin = list(set(final_parent_asin))\n",
    "        final_user_id_2_index = {final_user_id[i]: i for i in range(len(final_user_id))}\n",
    "        final_parent_asin_2_index = {final_parent_asin[i]: i for i in range(len(final_parent_asin))}\n",
    "        index_2_final_user_id = {i: final_user_id[i] for i in range(len(final_user_id))}\n",
    "        index_2_final_parent_asin = {i: final_parent_asin[i] for i in range(len(final_parent_asin))}\n",
    "        print('物品数：', len(final_parent_asin))\n",
    "        with open(os.path.join(save_dir, 'user_id_2_index.json'), 'w') as f:\n",
    "            json.dump(final_user_id_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "            json.dump(final_parent_asin_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_user_id.json'), 'w') as f:\n",
    "            json.dump(index_2_final_user_id, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "            json.dump(index_2_final_parent_asin, f)\n",
    "\n",
    "    def generate_dense_test_mapping(self, save_dir, num_users=500, min_interactions=5, train_user=None):\n",
    "        \"\"\"\n",
    "        筛选出具有密集交互模式的测试用户集合\n",
    "\n",
    "        Args:\n",
    "            save_dir: 保存映射文件的目录\n",
    "            num_users: 需要筛选的用户数量\n",
    "            min_interactions: 每个用户至少要有的交互次数\n",
    "            train_user: 训练集中的用户ID映射，这些用户将被排除\n",
    "\n",
    "        Returns:\n",
    "            筛选出的用户ID列表\n",
    "        \"\"\"\n",
    "        print(f\"开始筛选密集交互的{num_users}个测试用户...\")\n",
    "\n",
    "        # 创建保存目录\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # 读取数据\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "\n",
    "        # 排除训练集中的用户\n",
    "        if train_user is not None:\n",
    "            data = data[~data['user_id'].isin(train_user.keys())]\n",
    "            print(f\"排除训练集用户后剩余{len(data['user_id'].unique())}个用户\")\n",
    "\n",
    "        # 1. 统计每个用户的总交互次数（当前交互 + 历史交互）\n",
    "        user_interaction_count = {}\n",
    "        for _, row in data.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            history = str(row['history']).split()  # 确保历史是字符串并分割\n",
    "\n",
    "            # 计算总交互次数：历史交互数量 + 当前交互(1)\n",
    "            total_interactions = len(history) + 1\n",
    "\n",
    "            if user_id in user_interaction_count:\n",
    "                user_interaction_count[user_id] = max(user_interaction_count[user_id], total_interactions)\n",
    "            else:\n",
    "                user_interaction_count[user_id] = total_interactions\n",
    "\n",
    "        # 2. 筛选出交互次数达到阈值的用户\n",
    "        active_users = [user for user, count in user_interaction_count.items() if count >= min_interactions]\n",
    "        print(f\"交互次数>={min_interactions}的用户有{len(active_users)}个\")\n",
    "\n",
    "        if len(active_users) < num_users:\n",
    "            print(f\"警告: 符合条件的用户不足{num_users}个，将使用所有{len(active_users)}个符合条件的用户\")\n",
    "            num_users = len(active_users)\n",
    "\n",
    "        # 3. 构建物品流行度映射，便于后续分析\n",
    "        all_items = []\n",
    "        for _, row in data.iterrows():\n",
    "            if row['user_id'] in active_users:\n",
    "                all_items.append(row['parent_asin'])  # 添加当前交互物品\n",
    "                history = str(row['history']).split()\n",
    "                all_items.extend(history)  # 添加历史交互物品\n",
    "\n",
    "        item_popularity = {}\n",
    "        for item in all_items:\n",
    "            item_popularity[item] = item_popularity.get(item, 0) + 1\n",
    "\n",
    "        # 4. 为每个用户计算交互的物品集合\n",
    "        user_item_sets = {}\n",
    "        for _, row in data.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            if user_id in active_users:\n",
    "                if user_id not in user_item_sets:\n",
    "                    user_item_sets[user_id] = set()\n",
    "\n",
    "                user_item_sets[user_id].add(row['parent_asin'])  # 添加当前交互\n",
    "\n",
    "                # 添加历史交互\n",
    "                history = str(row['history']).split()\n",
    "                user_item_sets[user_id].update(history)\n",
    "\n",
    "        # 5. 计算用户间的物品重叠度\n",
    "        user_overlap_scores = {}\n",
    "        for user in active_users:\n",
    "            overlap_score = 0\n",
    "            user_items = user_item_sets.get(user, set())\n",
    "\n",
    "            for other_user in active_users:\n",
    "                if other_user != user:\n",
    "                    other_items = user_item_sets.get(other_user, set())\n",
    "                    # 计算Jaccard相似度\n",
    "                    intersection = len(user_items.intersection(other_items))\n",
    "                    union = len(user_items.union(other_items))\n",
    "                    if union > 0:\n",
    "                        overlap_score += intersection / union\n",
    "\n",
    "            # 平均重叠度\n",
    "            user_overlap_scores[user] = overlap_score / (len(active_users) - 1) if len(active_users) > 1 else 0\n",
    "\n",
    "        # 6. 根据重叠度排序用户\n",
    "        sorted_users = sorted(user_overlap_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 7. 使用改进的贪心算法选择用户\n",
    "        selected_users = []\n",
    "        all_selected_items = set()\n",
    "        candidate_items_count = {}  # 统计每个物品被多少候选用户交互\n",
    "\n",
    "        # 首先统计每个物品被多少活跃用户交互\n",
    "        for user in active_users:\n",
    "            for item in user_item_sets.get(user, set()):\n",
    "                candidate_items_count[item] = candidate_items_count.get(item, 0) + 1\n",
    "\n",
    "        # 选择一个种子用户，优先选择与高重叠度的用户\n",
    "        seed_user = sorted_users[0][0]\n",
    "        selected_users.append(seed_user)\n",
    "        all_selected_items.update(user_item_sets[seed_user])\n",
    "\n",
    "        # 定义一个函数计算用户的贡献度\n",
    "        def calculate_contribution(user, selected_items, user_items):\n",
    "            # 计算两个重要因素：\n",
    "            # 1. 与已选物品的重叠度\n",
    "            overlap_count = len(user_items.intersection(selected_items))\n",
    "            # 2. 物品的流行度加权分数\n",
    "            popularity_score = sum(item_popularity.get(item, 0) for item in user_items)\n",
    "            # 3. 交互的物品数量\n",
    "            item_count = len(user_items)\n",
    "\n",
    "            # 结合这些因素计算贡献度\n",
    "            if len(selected_items) == 0:\n",
    "                overlap_ratio = 0\n",
    "            else:\n",
    "                overlap_ratio = overlap_count / len(selected_items)\n",
    "\n",
    "            # 调整权重以控制密集度\n",
    "            overlap_weight = 0.7  # 重叠度权重\n",
    "            popularity_weight = 0.2  # 流行度权重\n",
    "            count_weight = 0.1  # 物品数量权重\n",
    "\n",
    "            return (overlap_ratio * overlap_weight +\n",
    "                    (popularity_score / (item_count or 1)) * popularity_weight +\n",
    "                    item_count * count_weight)\n",
    "\n",
    "        # 贪心选择剩余用户\n",
    "        remaining_users = [user for user, _ in sorted_users if user != seed_user]\n",
    "\n",
    "        while len(selected_users) < num_users and remaining_users:\n",
    "            max_contribution = -1\n",
    "            best_user = None\n",
    "\n",
    "            for user in remaining_users:\n",
    "                user_items = user_item_sets.get(user, set())\n",
    "                contribution = calculate_contribution(user, all_selected_items, user_items)\n",
    "\n",
    "                if contribution > max_contribution:\n",
    "                    max_contribution = contribution\n",
    "                    best_user = user\n",
    "\n",
    "            if best_user:\n",
    "                selected_users.append(best_user)\n",
    "                all_selected_items.update(user_item_sets[best_user])\n",
    "                remaining_users.remove(best_user)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        print(f\"成功选择{len(selected_users)}个用户\")\n",
    "\n",
    "        # 8. 获取这些用户交互的所有物品\n",
    "        final_parent_asin = set()\n",
    "        user_item_count = {}  # 记录每个用户交互的物品数量\n",
    "\n",
    "        for user in selected_users:\n",
    "            user_items = set()\n",
    "            user_rows = data[data['user_id'] == user]\n",
    "\n",
    "            for _, row in user_rows.iterrows():\n",
    "                # 添加当前交互物品\n",
    "                current_item = row['parent_asin']\n",
    "                final_parent_asin.add(current_item)\n",
    "                user_items.add(current_item)\n",
    "\n",
    "                # 添加历史交互物品\n",
    "                history = str(row['history']).split()\n",
    "                final_parent_asin.update(history)\n",
    "                user_items.update(history)\n",
    "\n",
    "            user_item_count[user] = len(user_items)\n",
    "\n",
    "        final_parent_asin = list(final_parent_asin)\n",
    "\n",
    "        # 9. 创建并保存映射\n",
    "        final_user_id_2_index = {selected_users[i]: i for i in range(len(selected_users))}\n",
    "        final_parent_asin_2_index = {final_parent_asin[i]: i for i in range(len(final_parent_asin))}\n",
    "        index_2_final_user_id = {i: selected_users[i] for i in range(len(selected_users))}\n",
    "        index_2_final_parent_asin = {i: final_parent_asin[i] for i in range(len(final_parent_asin))}\n",
    "\n",
    "        # 10. 计算统计信息\n",
    "        total_interactions = sum(user_item_count.values())\n",
    "        distinct_interactions = len(final_parent_asin)\n",
    "        density = total_interactions / (len(selected_users) * distinct_interactions) if distinct_interactions > 0 else 0\n",
    "\n",
    "        print(\"\\n=== 数据集统计 ===\")\n",
    "        print(f\"用户数: {len(selected_users)}\")\n",
    "        print(f\"物品数: {distinct_interactions}\")\n",
    "        print(f\"总交互数: {total_interactions}\")\n",
    "        print(f\"交互密度: {density:.4f}\")\n",
    "        print(f\"平均每用户交互物品数: {total_interactions/len(selected_users):.2f}\")\n",
    "        print(f\"物品被交互的平均次数: {total_interactions/distinct_interactions:.2f}\")\n",
    "\n",
    "        # 绘制交互分布直方图\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(list(user_item_count.values()), bins=20)\n",
    "            plt.title('用户交互数分布')\n",
    "            plt.xlabel('交互数')\n",
    "            plt.ylabel('用户数')\n",
    "            plt.savefig(os.path.join(save_dir, 'user_interaction_distribution.png'))\n",
    "            plt.close()\n",
    "        except:\n",
    "            print(\"无法生成分布图\")\n",
    "\n",
    "        # 11. 保存映射文件\n",
    "        with open(os.path.join(save_dir, 'user_id_2_index.json'), 'w') as f:\n",
    "            json.dump(final_user_id_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "            json.dump(final_parent_asin_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_user_id.json'), 'w') as f:\n",
    "            json.dump(index_2_final_user_id, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "            json.dump(index_2_final_parent_asin, f)\n",
    "\n",
    "        return selected_users\n",
    "\n",
    "    def print_jsonl_data_keys(self, path):\n",
    "        \"\"\"\n",
    "        Print the keys of jsonl data\n",
    "        :param path: str, the path of the jsonl file\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                print(list(data.keys()))\n",
    "                break\n",
    "\n",
    "    def transform_meta_jsonl_to_parquet(self, mapping_dir, save_dir):\n",
    "        final_parent_asin_2_index = json.load(open(os.path.join(mapping_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "\n",
    "        data = []\n",
    "        final_parent_asin = final_parent_asin_2_index.keys()\n",
    "        # keep the columns we need\n",
    "        # all: ['main_category', 'title', 'average_rating', 'rating_number', 'features', 'description', 'price', 'images', 'videos', 'store', 'categories', 'details', 'parent_asin', 'bought_together']\n",
    "        # needs: ['main_category', 'title', 'average_rating', 'rating_number', 'price', 'images', 'store', 'categories', 'details', 'parent_asin']\n",
    "        with open(self.raw_meta_data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['parent_asin'] in final_parent_asin:\n",
    "                    data.append(json.loads(line))\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data[['main_category', 'title', 'average_rating', 'rating_number', 'price', 'images', 'store', 'categories', 'details', 'parent_asin']]\n",
    "        # keep the data whose parent_asin is in the interaction data\n",
    "        data = data[data['parent_asin'].isin(final_parent_asin)]\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: final_parent_asin_2_index[x])\n",
    "        data.to_parquet(os.path.join(save_dir, 'meta_data.parquet'))\n",
    "\n",
    "    def transform_review_jsonl_to_parquet(self, mapping_dir, save_dir):\n",
    "        final_user_id_2_index = json.load(open(os.path.join(mapping_dir, 'user_id_2_index.json'), 'r'))\n",
    "        final_parent_asin_2_index = json.load(open(os.path.join(mapping_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "        data = []\n",
    "        user_id = set(final_user_id_2_index.keys())\n",
    "        parent_asin = set(final_parent_asin_2_index.keys())\n",
    "        # keep the columns we need\n",
    "        # all: ['rating', 'title', 'text', 'images', 'asin', 'parent_asin', 'user_id', 'timestamp', 'helpful_vote', 'verified_purchase']\n",
    "        # needs: ['parent_asin', 'user_id', 'text', 'timestamp', 'rating']\n",
    "        with open(self.raw_review_data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if json.loads(line)['parent_asin'] in parent_asin and json.loads(line)['user_id'] in user_id:\n",
    "                    data.append(json.loads(line))\n",
    "        data = pd.DataFrame(data)\n",
    "        data = data[['parent_asin', 'user_id', 'text', 'timestamp', 'rating']]\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: final_parent_asin_2_index[x])\n",
    "        data['user_id'] = data['user_id'].apply(lambda x: final_user_id_2_index[x] if x in final_user_id_2_index else -1)\n",
    "        data.to_parquet(os.path.join(save_dir, 'review_data.parquet'))\n",
    "\n",
    "    def generate_test_data(self, save_dir):\n",
    "        final_user_id_2_index = json.load(open(os.path.join(save_dir, 'user_id_2_index.json'), 'r'))\n",
    "        final_parent_asin_2_index = json.load(open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "        # keep the data whose user_id is in the 100 users\n",
    "        data = data[data['user_id'].isin(final_user_id_2_index.keys())]\n",
    "        data['user_id'] = data['user_id'].apply(lambda x: final_user_id_2_index[x])\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: final_parent_asin_2_index[x])\n",
    "        data['history'] = data['history'].apply(lambda x: [final_parent_asin_2_index[i] for i in x.split()])\n",
    "        # data.to_parquet(os.path.join(save_dir, 'test_data.parquet'))\n",
    "        data.to_parquet('baby_data/test_data.parquet')\n",
    "\n",
    "    def generate_dense_test_mapping_fast(self, save_dir, num_users=500, min_interactions=5, train_user=None, sampling_ratio=0.2, max_items=2000, min_item_users=5):\n",
    "        \"\"\"\n",
    "        使用更高效的方法筛选出具有密集交互模式的测试用户集合，并限制物品数量\n",
    "\n",
    "        Args:\n",
    "            save_dir: 保存映射文件的目录\n",
    "            num_users: 需要筛选的用户数量\n",
    "            min_interactions: 每个用户至少要有的交互次数\n",
    "            train_user: 训练集中的用户ID映射，这些用户将被排除\n",
    "            sampling_ratio: 如果用户数量巨大，使用此比例进行抽样处理\n",
    "            max_items: 最大物品数量限制\n",
    "            min_item_users: 物品至少被多少用户交互才保留\n",
    "\n",
    "        Returns:\n",
    "            筛选出的用户ID列表\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from collections import Counter\n",
    "        from sklearn.cluster import MiniBatchKMeans\n",
    "        from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "        print(f\"开始高效筛选密集交互的{num_users}个测试用户...\")\n",
    "\n",
    "        # 创建保存目录\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # 读取数据\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "        total_users = len(data['user_id'].unique())\n",
    "\n",
    "        # 排除训练集中的用户\n",
    "        if train_user is not None:\n",
    "            data = data[~data['user_id'].isin(train_user.keys())]\n",
    "            print(f\"排除训练集用户后剩余{len(data['user_id'].unique())}/{total_users}个用户\")\n",
    "\n",
    "        # 如果用户数量太多，考虑随机抽样\n",
    "        if len(data['user_id'].unique()) > 10000 and sampling_ratio < 1.0:\n",
    "            sample_users = np.random.choice(\n",
    "                data['user_id'].unique(),\n",
    "                size=int(len(data['user_id'].unique()) * sampling_ratio),\n",
    "                replace=False\n",
    "            )\n",
    "            data = data[data['user_id'].isin(sample_users)]\n",
    "            print(f\"随机抽样{sampling_ratio:.1%}的用户，现在有{len(data['user_id'].unique())}个用户\")\n",
    "\n",
    "        # 1. 创建用户-物品交互字典和物品流行度计数器\n",
    "        user_items = {}\n",
    "        item_users = Counter()  # 记录每个物品被多少用户交互\n",
    "        item_count = Counter()  # 记录每个物品的总交互次数\n",
    "\n",
    "        # 一次性处理所有行，提取交互信息\n",
    "        print(\"处理用户交互数据...\")\n",
    "        for _, row in data.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            if user_id not in user_items:\n",
    "                user_items[user_id] = set()\n",
    "\n",
    "            # 添加当前交互物品\n",
    "            current_item = row['parent_asin']\n",
    "            user_items[user_id].add(current_item)\n",
    "            item_count[current_item] += 1\n",
    "\n",
    "            # 添加历史交互物品\n",
    "            history = str(row['history']).split()\n",
    "            for item in history:\n",
    "                user_items[user_id].add(item)\n",
    "                item_count[item] += 1\n",
    "\n",
    "        # 更新物品被多少用户交互的计数\n",
    "        for user, items in user_items.items():\n",
    "            for item in items:\n",
    "                item_users[item] += 1\n",
    "\n",
    "        # 2. 筛选出交互次数达到阈值的用户\n",
    "        active_users = [user for user, items in user_items.items() if len(items) >= min_interactions]\n",
    "        print(f\"交互次数>={min_interactions}的用户有{len(active_users)}个\")\n",
    "\n",
    "        if len(active_users) < num_users:\n",
    "            print(f\"警告: 符合条件的用户不足{num_users}个，将使用所有{len(active_users)}个符合条件的用户\")\n",
    "            num_users = len(active_users)\n",
    "\n",
    "        # 3. 筛选物品 - 保留被足够多用户交互且流行度高的物品\n",
    "        print(f\"原始物品总数: {len(item_users)}\")\n",
    "\n",
    "        # 先筛选掉被交互用户数少于阈值的物品\n",
    "        popular_items = {item: count for item, count in item_users.items() if count >= min_item_users}\n",
    "        print(f\"被至少{min_item_users}个用户交互的物品数: {len(popular_items)}\")\n",
    "\n",
    "        # 如果物品仍然过多，按流行度选择top物品\n",
    "        if len(popular_items) > max_items:\n",
    "            # 按交互用户数排序\n",
    "            sorted_items = sorted(popular_items.items(), key=lambda x: (x[1], item_count[x[0]]), reverse=True)\n",
    "            popular_items = {item: count for item, count in sorted_items[:max_items]}\n",
    "            print(f\"限制为流行度最高的{max_items}个物品\")\n",
    "\n",
    "        # 4. 更新用户的物品集合，只保留筛选后的物品\n",
    "        filtered_user_items = {}\n",
    "        for user, items in user_items.items():\n",
    "            filtered_items = {item for item in items if item in popular_items}\n",
    "            if len(filtered_items) >= min_interactions:  # 确保用户仍有足够的交互\n",
    "                filtered_user_items[user] = filtered_items\n",
    "\n",
    "        # 更新活跃用户列表\n",
    "        active_users = list(filtered_user_items.keys())\n",
    "        print(f\"筛选物品后，仍有{len(active_users)}个用户有足够交互\")\n",
    "\n",
    "        if len(active_users) < num_users:\n",
    "            print(f\"警告: 筛选物品后，符合条件的用户不足{num_users}个\")\n",
    "            num_users = len(active_users)\n",
    "\n",
    "        # 5. 映射物品ID到数字索引\n",
    "        item_id_map = {}\n",
    "        next_item_id = 0\n",
    "        for items in filtered_user_items.values():\n",
    "            for item_id in items:\n",
    "                if item_id not in item_id_map:\n",
    "                    item_id_map[item_id] = next_item_id\n",
    "                    next_item_id += 1\n",
    "\n",
    "        # 6. 构建用户-物品矩阵 (稀疏表示)\n",
    "        print(\"构建用户-物品矩阵...\")\n",
    "        user_to_idx = {user: i for i, user in enumerate(active_users)}\n",
    "\n",
    "        # 创建稀疏矩阵的行、列和值列表\n",
    "        rows, cols, data_values = [], [], []\n",
    "\n",
    "        for user, items in filtered_user_items.items():\n",
    "            if user in user_to_idx:  # 只考虑活跃用户\n",
    "                u_idx = user_to_idx[user]\n",
    "                for item in items:\n",
    "                    i_idx = item_id_map[item]\n",
    "                    rows.append(u_idx)\n",
    "                    cols.append(i_idx)\n",
    "                    data_values.append(1)  # 二元交互\n",
    "\n",
    "        # 创建稀疏矩阵\n",
    "        from scipy.sparse import csr_matrix\n",
    "        user_item_matrix = csr_matrix((data_values, (rows, cols)),\n",
    "                                      shape=(len(active_users), len(item_id_map)))\n",
    "\n",
    "        # 7. 使用TF-IDF变换矩阵，突出重要物品\n",
    "        print(\"应用TF-IDF变换...\")\n",
    "        transformer = TfidfTransformer()\n",
    "        user_item_tfidf = transformer.fit_transform(user_item_matrix)\n",
    "\n",
    "        # 8. 使用MiniBatchKMeans进行快速聚类\n",
    "        print(\"聚类用户...\")\n",
    "        # 尝试将用户聚为多个类别，选择一些最大的簇\n",
    "        n_clusters = min(int(num_users / 5), len(active_users) // 10)\n",
    "        n_clusters = max(n_clusters, 5)  # 至少5个簇\n",
    "\n",
    "        kmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=42)\n",
    "        user_clusters = kmeans.fit_predict(user_item_tfidf)\n",
    "\n",
    "        # 9. 计算每个簇的大小\n",
    "        cluster_sizes = Counter(user_clusters)\n",
    "\n",
    "        # 10. 从每个簇中选择具有典型交互模式的用户\n",
    "        print(\"从聚类中选择用户...\")\n",
    "        selected_users = []\n",
    "\n",
    "        # 按簇大小降序排列\n",
    "        sorted_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 从每个簇中选择用户数量与簇大小成比例\n",
    "        remaining = num_users\n",
    "        for cluster_id, cluster_size in sorted_clusters:\n",
    "            # 确定从该簇中选择的用户数\n",
    "            to_select = max(1, int(remaining * (cluster_size / sum([s for _, s in sorted_clusters]))))\n",
    "            to_select = min(to_select, cluster_size, remaining)\n",
    "\n",
    "            # 获取该簇的所有用户\n",
    "            cluster_users = [active_users[i] for i, c in enumerate(user_clusters) if c == cluster_id]\n",
    "\n",
    "            # 选择该簇中交互物品数量最多的用户\n",
    "            users_with_counts = [(user, len(filtered_user_items[user])) for user in cluster_users]\n",
    "            users_with_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # 选择前to_select个用户\n",
    "            selected_from_cluster = [user for user, _ in users_with_counts[:to_select]]\n",
    "            selected_users.extend(selected_from_cluster)\n",
    "\n",
    "            remaining -= len(selected_from_cluster)\n",
    "            if remaining <= 0:\n",
    "                break\n",
    "\n",
    "        # 如果还需要更多用户，从剩余的活跃用户中选择\n",
    "        if len(selected_users) < num_users:\n",
    "            remaining_users = [u for u in active_users if u not in selected_users]\n",
    "            # 按交互物品数量排序\n",
    "            remaining_with_counts = [(user, len(filtered_user_items[user])) for user in remaining_users]\n",
    "            remaining_with_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            additional = min(num_users - len(selected_users), len(remaining_users))\n",
    "            selected_users.extend([user for user, _ in remaining_with_counts[:additional]])\n",
    "\n",
    "        print(f\"成功选择{len(selected_users)}个用户\")\n",
    "\n",
    "        # 11. 获取这些用户交互的所有物品\n",
    "        final_parent_asin = set()\n",
    "        user_item_count = {}  # 记录每个用户交互的物品数量\n",
    "\n",
    "        for user in selected_users:\n",
    "            user_items_set = filtered_user_items[user]\n",
    "            final_parent_asin.update(user_items_set)\n",
    "            user_item_count[user] = len(user_items_set)\n",
    "\n",
    "        final_parent_asin = list(final_parent_asin)\n",
    "\n",
    "        # 12. 创建并保存映射\n",
    "        final_user_id_2_index = {selected_users[i]: i for i in range(len(selected_users))}\n",
    "        final_parent_asin_2_index = {final_parent_asin[i]: i for i in range(len(final_parent_asin))}\n",
    "        index_2_final_user_id = {i: selected_users[i] for i in range(len(selected_users))}\n",
    "        index_2_final_parent_asin = {i: final_parent_asin[i] for i in range(len(final_parent_asin))}\n",
    "\n",
    "        # 13. 计算统计信息\n",
    "        total_interactions = sum(user_item_count.values())\n",
    "        distinct_interactions = len(final_parent_asin)\n",
    "        density = total_interactions / (len(selected_users) * distinct_interactions) if distinct_interactions > 0 else 0\n",
    "\n",
    "        print(\"\\n=== 数据集统计 ===\")\n",
    "        print(f\"用户数: {len(selected_users)}\")\n",
    "        print(f\"物品数: {distinct_interactions}\")\n",
    "        print(f\"总交互数: {total_interactions}\")\n",
    "        print(f\"交互密度: {density:.4f}\")\n",
    "        print(f\"平均每用户交互物品数: {total_interactions/len(selected_users):.2f}\")\n",
    "        print(f\"物品被交互的平均次数: {total_interactions/distinct_interactions:.2f}\")\n",
    "\n",
    "        # 13. 保存映射文件\n",
    "        with open(os.path.join(save_dir, 'user_id_2_index.json'), 'w') as f:\n",
    "            json.dump(final_user_id_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "            json.dump(final_parent_asin_2_index, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_user_id.json'), 'w') as f:\n",
    "            json.dump(index_2_final_user_id, f)\n",
    "        with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "            json.dump(index_2_final_parent_asin, f)\n",
    "\n",
    "        return selected_users\n",
    "\n",
    "    def generate_test_data_new(self, save_dir):\n",
    "        \"\"\"\n",
    "        从交互数据中提取测试集，确保所有映射中的用户都被包含，\n",
    "        如果用户当前交互的物品(parent_asin)不在映射中，则将其添加到映射中\n",
    "        \n",
    "        Args:\n",
    "            save_dir: 保存目录，应包含user_id_2_index.json和parent_asin_2_index.json文件\n",
    "        \"\"\"\n",
    "        # 加载映射文件\n",
    "        final_user_id_2_index = json.load(open(os.path.join(save_dir, 'user_id_2_index.json'), 'r'))\n",
    "        final_parent_asin_2_index = json.load(open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'r'))\n",
    "        index_2_final_parent_asin = json.load(open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'r'))\n",
    "        \n",
    "        # 输出一共多少个用户和物品\n",
    "        print(f\"原始映射中的用户数量: {len(set(final_user_id_2_index.keys()))}\")\n",
    "        print(f\"原始映射中的物品数量: {len(set(final_parent_asin_2_index.keys()))}\")\n",
    "        \n",
    "        # 创建物品ID集合，用于快速检查\n",
    "        valid_items = set(final_parent_asin_2_index.keys())\n",
    "        \n",
    "        # 读取交互数据\n",
    "        data = pd.read_csv(self.test_interaction_data_path)\n",
    "        \n",
    "        # 只保留映射中的用户\n",
    "        data = data[data['user_id'].isin(final_user_id_2_index.keys())]\n",
    "        print(f\"映射中的用户交互数据: {len(data)} 条记录\")\n",
    "        \n",
    "        # 检查是否有用户的当前交互物品不在映射中\n",
    "        missing_items = set()\n",
    "        for _, row in data.iterrows():\n",
    "            if row['parent_asin'] not in valid_items:\n",
    "                missing_items.add(row['parent_asin'])\n",
    "        \n",
    "        # 将缺失的物品添加到映射中\n",
    "        if missing_items:\n",
    "            print(f\"发现 {len(missing_items)} 个用户当前交互物品不在映射中，添加这些物品\")\n",
    "            \n",
    "            # 获取当前最大索引\n",
    "            max_idx = max([int(idx) for idx in index_2_final_parent_asin.keys()])\n",
    "            \n",
    "            # 为缺失物品分配新索引\n",
    "            new_idx = max_idx + 1\n",
    "            for item in missing_items:\n",
    "                final_parent_asin_2_index[item] = new_idx\n",
    "                index_2_final_parent_asin[str(new_idx)] = item  # 注意这里的键需要是字符串\n",
    "                new_idx += 1\n",
    "            \n",
    "            # 更新有效物品集合\n",
    "            valid_items = set(final_parent_asin_2_index.keys())\n",
    "            \n",
    "            # 保存更新后的映射\n",
    "            with open(os.path.join(save_dir, 'parent_asin_2_index.json'), 'w') as f:\n",
    "                json.dump(final_parent_asin_2_index, f)\n",
    "            with open(os.path.join(save_dir, 'index_2_parent_asin.json'), 'w') as f:\n",
    "                json.dump(index_2_final_parent_asin, f)\n",
    "            \n",
    "            print(f\"更新后的物品数量: {len(valid_items)}\")\n",
    "        \n",
    "        # 将user_id映射到索引\n",
    "        data['user_id'] = data['user_id'].apply(lambda x: final_user_id_2_index[x])\n",
    "        \n",
    "        # 将parent_asin映射到索引\n",
    "        data['parent_asin'] = data['parent_asin'].apply(lambda x: final_parent_asin_2_index[x])\n",
    "        \n",
    "        # 处理历史记录，过滤掉不在映射中的物品\n",
    "        def filter_history(history_str):\n",
    "            history_items = str(history_str).split()\n",
    "            filtered_history = [final_parent_asin_2_index[item] for item in history_items \n",
    "                            if item in valid_items]\n",
    "            return filtered_history\n",
    "        \n",
    "        # 应用历史记录过滤\n",
    "        data['history'] = data['history'].apply(filter_history)\n",
    "        \n",
    "        # 检查处理后的数据\n",
    "        print(f\"处理后的测试集大小: {len(data)} 条记录\")\n",
    "        print(f\"包含的用户数量: {data['user_id'].nunique()}\")\n",
    "        print(f\"包含的物品数量: {data['parent_asin'].nunique()}\")\n",
    "        \n",
    "        # 计算历史记录中的物品数量\n",
    "        history_items = set()\n",
    "        for hist in data['history']:\n",
    "            history_items.update(hist)\n",
    "        print(f\"历史记录中的物品索引数量: {len(history_items)}\")\n",
    "        \n",
    "        # 保存处理后的数据\n",
    "        output_path = os.path.join(save_dir, 'test_data.parquet')\n",
    "        data.to_parquet(output_path)\n",
    "        print(f\"测试数据已保存到: {output_path}\")\n",
    "        \n",
    "        # return data\n",
    "\n",
    "# raw_data_processor_to_100user = Raw_Data_Processor_to_100user(\n",
    "#     raw_meta_data_path='raw_data/meta_Baby_Products.jsonl',\n",
    "#     raw_review_data_path='raw_data/Baby_Products.jsonl',\n",
    "#     test_interaction_data_path='raw_data/Baby_Products.test.csv'\n",
    "# )\n",
    "# raw_data_processor_to_100user = Raw_Data_Processor_to_100user(\n",
    "#     raw_meta_data_path='raw_data/meta_Video_Games.jsonl',\n",
    "#     raw_review_data_path='raw_data/Video_Games.jsonl',\n",
    "#     test_interaction_data_path='raw_data/Video_Games.test.csv'\n",
    "# )\n",
    "raw_data_processor_to_100user = Raw_Data_Processor_to_100user(\n",
    "    raw_meta_data_path='raw_data/meta_CDs_and_Vinyl.jsonl',\n",
    "    raw_review_data_path='raw_data/CDs_and_Vinyl.jsonl',\n",
    "    test_interaction_data_path='raw_data/CDs_and_Vinyl.test.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_processor_to_100user.generate_mapping('cd_data/train')\n",
    "raw_data_processor_to_100user.generate_test_mapping('cd_data/test', 500, json.load(open('cd_data/train/user_id_2_index.json', 'r')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_processor_to_100user.print_jsonl_data_keys('raw_data/meta_Video_Games.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_processor_to_100user.transform_meta_jsonl_to_parquet('cd_data/test', 'cd_data/test')\n",
    "raw_data_processor_to_100user.transform_review_jsonl_to_parquet('cd_data/test', 'cd_data/test')\n",
    "raw_data_processor_to_100user.generate_test_data('cd_data/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
